{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Session 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(0)\n",
    "CRED = '\\033[91m'\n",
    "CEND = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Different distance measures in KNN (1 point)\n",
    "\n",
    "The goal of this task is to understand better how different distance measures can affect the KNN performance. In addition, we are going to test the effect of normalizing input features and using weighted averaging for the nearest neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Load the \"wine\" dataset from sklearn datasets, put input features into pandas dataframe and name the columns with feature names. <br>\n",
    "Report the number of features and the number of classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "wine = datasets.load_wine()\n",
    "#HINTS:\n",
    "#wine.data contains numpy array of dataset input features\n",
    "#wine.target contains labels of instances\n",
    "#wine.feature_names contains features labels\n",
    "\n",
    "try:\n",
    "    ##### YOUR CODE STARTS ##### (please do not delete this line)\n",
    "    raise NotImplementedError(\"You need to remove \\\"raise NotImplementedErrors\\\" lines after writing your code!\")\n",
    "    wine_train = pd.DataFrame(data = ..., columns = ...)\n",
    "    display(wine_train.head())\n",
    "    ##### YOUR CODE ENDS ##### (please do not delete this line)\n",
    "except NotImplementedError as e:\n",
    "    print(CRED,\"TODO:\",e,CEND)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (a):</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Split the dataset into 80% training data and 20% test data using random seed = 22, and report the number of instances in the training set and in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ##### YOUR CODE STARTS ##### (please do not delete this line)\n",
    "    raise NotImplementedError(\"You need to remove \\\"raise NotImplementedErrors\\\" lines after writing your code!\")\n",
    "    ##### YOUR CODE ENDS ##### (please do not delete this line)\n",
    "except NotImplementedError as e:\n",
    "    print(CRED,\"TODO:\",e,CEND)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (b):</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Use the KNN Classifier from Sklearn to fit a model for different values of K (3, 5, 7). Report test accuracy for each K. Report the best K to be used on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ##### YOUR CODE STARTS ##### (please do not delete this line)\n",
    "    raise NotImplementedError(\"You need to remove \\\"raise NotImplementedErrors\\\" lines after writing your code!\")\n",
    "    ##### YOUR CODE ENDS ##### (please do not delete this line)\n",
    "except NotImplementedError as e:\n",
    "    print(CRED,\"TODO:\",e,CEND)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (c):</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Use the best value of K from the previous subtask, and try changing the distance metric used by KNN into euclidean, manhattan, chebyshev and minkowski with P = 3. Report the accuracy obtained with each metric. Report the best metric to be used on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ##### YOUR CODE STARTS ##### (please do not delete this line)\n",
    "    raise NotImplementedError(\"You need to remove \\\"raise NotImplementedErrors\\\" lines after writing your code!\")\n",
    "    ##### YOUR CODE ENDS ##### (please do not delete this line)\n",
    "except NotImplementedError as e:\n",
    "    print(CRED,\"TODO:\",e,CEND)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (d):</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** Try to use weights for nearest neighbors according to the inverse of their distance from the test instance. Use the best K found in **(c)** and the best distance metric found in **(d)**. Report the accuracy and discuss whether it is better to use weights or not. (Hint: Read about `weights` parameter from <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">HERE</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ##### YOUR CODE STARTS ##### (please do not delete this line)\n",
    "    raise NotImplementedError(\"You need to remove \\\"raise NotImplementedErrors\\\" lines after writing your code!\")\n",
    "    ##### YOUR CODE ENDS ##### (please do not delete this line)\n",
    "except NotImplementedError as e:\n",
    "    print(CRED,\"TODO:\",e,CEND)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (e):</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)** Standardize the features in the whole dataset by subtracting the mean and dividing by standard deviation from each feature (The mean and standard deviation should be calculated from the training instances only). Predict the accuracy using best parameters found in **(c)**, **(d)**, and **(e)**. Is it better to standardize / normalize your features before using KNN? WHY?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ##### YOUR CODE STARTS ##### (please do not delete this line)\n",
    "    raise NotImplementedError(\"You need to remove \\\"raise NotImplementedErrors\\\" lines after writing your code!\")\n",
    "    ##### YOUR CODE ENDS ##### (please do not delete this line)\n",
    "except NotImplementedError as e:\n",
    "    print(CRED,\"TODO:\",e,CEND)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (f):</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g)** Based on your observations during this task, write out what have you learnt so far when using the KNN learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (g):</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Linear and polynomial kernels in SVM (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this task is to understand better how the choice of kernel and parameter values affect the performance of SVM. You are given a two-dimensional dataset with a binary label and two features: $x_1$ and $x_2$. Your class labels are generated from the features using the XOR function, such that the class is positive when either $x_1 > 0$ or $x_2 > 0$ but not both. The following code creates this dataset and plots it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = np.random.randn(300, 2)\n",
    "train_Y = np.logical_xor(train_X[:, 0] > 0, train_X[:, 1] > 0)\n",
    "\n",
    "plt.scatter(train_X[:, 0], train_X[:, 1], s=30, c=train_Y, cmap=plt.cm.Paired,\n",
    "            edgecolors='k')\n",
    "plt.axis([-3, 3, -3, 3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(a)** Let's first learn SVM with the linear kernel. Please fill in the 2 gaps in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ##### YOUR CODE STARTS ##### (please do not delete this line)\n",
    "    raise NotImplementedError(\"You need to remove \\\"raise NotImplementedErrors\\\" lines after writing your code!\")\n",
    "    # fit the model here:\n",
    "    svm_model = svm.SVC(...)\n",
    "    svm_model.fit(...)\n",
    "    ##### YOUR CODE ENDS ##### (please do not delete this line)\n",
    "except NotImplementedError as e:\n",
    "    print(CRED,\"TODO:\",e,CEND)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.linspace(-3, 3, 500),\n",
    "                     np.linspace(-3, 3, 500))\n",
    "\n",
    "def plot_svm(model, X, Y, ax = None):\n",
    "    # plots the decision function for each datapoint on the grid\n",
    "    \n",
    "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    if ax is None:\n",
    "        plt.imshow(Z, interpolation='nearest',\n",
    "                   extent=(xx.min(), xx.max(), yy.min(), yy.max()), aspect='auto',\n",
    "                   origin='lower', cmap=plt.cm.PuOr_r)\n",
    "        contours = plt.contour(xx, yy, Z, levels=[0], linewidths=2)\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired,\n",
    "                    edgecolors='k')\n",
    "        plt.axis([-3, 3, -3, 3])\n",
    "        plt.show()\n",
    "    else:\n",
    "        ax.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()), aspect='auto',\n",
    "           origin='lower', cmap=plt.cm.PuOr_r)\n",
    "        contours = ax.contour(xx, yy, Z, levels=[0], linewidths=2)\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired,\n",
    "                    edgecolors='k')\n",
    "        ax.axis([-3, 3, -3, 3])\n",
    "\n",
    "try:\n",
    "    plot_svm(svm_model, train_X, train_Y)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** Can SVM with the linear kernel separate these data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (b):</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(c)** Now learn the SVM model with the polynomial kernel of degree 2 and degree 3. Which of them is able to learn the XOR function (not perfectly but reasonably well)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ##### YOUR CODE STARTS ##### (please do not delete this line)\n",
    "    raise NotImplementedError(\"You need to remove \\\"raise NotImplementedErrors\\\" lines after writing your code!\")\n",
    "    # Degree 2\n",
    "    ##### YOUR CODE ENDS ##### (please do not delete this line)\n",
    "except NotImplementedError as e:\n",
    "    print(CRED,\"TODO:\",e,CEND)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ##### YOUR CODE STARTS ##### (please do not delete this line)\n",
    "    raise NotImplementedError(\"You need to remove \\\"raise NotImplementedErrors\\\" lines after writing your code!\")\n",
    "    # Degree 3\n",
    "    ##### YOUR CODE ENDS ##### (please do not delete this line)\n",
    "except NotImplementedError as e:\n",
    "    print(CRED,\"TODO:\",e,CEND)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (c):</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(d)** Let us try to understand the reason why one degree worked well and the other not. Which of the following features is the most useful in discriminating the two classes: $x_1$, $x_2$, $x_1^2$, $x_2^2$, $x_1^3$, $x_2^3$, $x_1 x_2$, $x_1^2 x_2$, $x_2^2 x_1$? (Hint: If you cannot guess the right answer then you could visualize these as a surface, example code for $x_2^2 x_1$ is shown below where lighter colour is a hill and darker is valley)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.contour(xx, yy, yy*yy*xx)\n",
    "plt.title('X1.X2^2')\n",
    "plt.show()\n",
    "try:\n",
    "    ##### YOUR CODE STARTS ##### (please do not delete this line)\n",
    "    raise NotImplementedError(\"You need to remove \\\"raise NotImplementedErrors\\\" lines after writing your code!\")\n",
    "    # Visualize rest of the features similarly as above\n",
    "    ##### YOUR CODE ENDS ##### (please do not delete this line)\n",
    "except NotImplementedError as e:\n",
    "    print(CRED,\"TODO:\",e,CEND)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (d):</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(e)** Expand the expression $\\kappa(\\mathbf{x},\\mathbf{z})=\\left(\\mathbf{x}\\cdot\\mathbf{z}\\right)^2$ (open the brackets) where $\\mathbf{x}=(x_1,x_2)$ and $\\mathbf{z}=(z_1,z_2)$. **<font color='purple'>For this please modify the following code for your purpose:</font>** Hint: we did this in the practice session, but you can follow a similar example at page 51 of Lecture 05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "sp.init_printing(use_latex='mathjax')\n",
    "\n",
    "try:\n",
    "    ##### YOUR CODE STARTS ##### (please do not delete this line)\n",
    "    raise NotImplementedError(\"You need to remove \\\"raise NotImplementedErrors\\\" lines after writing your code!\")\n",
    "    #Example\n",
    "    a, b = sp.symbols('a_1 b_2')\n",
    "    sp.expand( (a + b)**2 )\n",
    "    ##### YOUR CODE ENDS ##### (please do not delete this line)\n",
    "except NotImplementedError as e:\n",
    "    print(CRED,\"TODO:\",e,CEND)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(f)** Please now list all features that are included in the constructed feature space $\\phi(\\mathbf{x})$ corresponding to the kernel $\\kappa(\\mathbf{x},\\mathbf{z})=(\\mathbf{x}\\cdot\\mathbf{z})^2$. Hint: You can read this out from your answer to **(e)** because $\\kappa(\\mathbf{x},\\mathbf{z})=\\phi(\\mathbf{x})\\cdot\\phi(\\mathbf{z})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (f):</font>**\n",
    "$\\phi(x)=(...)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(g)** Is the best discriminator from subtask **(d)** present among the constructed features as listed in subtask **(f)**? Does this explain why polynomial kernel with degree 2 performed well / not well in subtask **(c)**? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (g):</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(h)** Please now expand the expression $\\kappa(\\mathbf{x},\\mathbf{z})=\\left(\\mathbf{x}\\cdot\\mathbf{z}\\right)^3$ similarly to subtask **(e)** and list the features in the constructed feature space $\\phi(\\mathbf{x})$ corresponding to this kernel, as in subtask **(f)**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ##### YOUR CODE STARTS ##### (please do not delete this line)\n",
    "    raise NotImplementedError(\"You need to remove \\\"raise NotImplementedErrors\\\" lines after writing your code!\")\n",
    "    ##### YOUR CODE ENDS ##### (please do not delete this line)\n",
    "except NotImplementedError as e:\n",
    "    print(CRED,\"TODO:\",e,CEND)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (h):</font>**\n",
    "$\\phi(x)=(...)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(i)** Is the best discriminator from subtask **(d)** present among the constructed features as listed in subtask **(h)**? Does this explain why polynomial kernel with degree 3 performed well / not well in subtask **(c)**? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (i):</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(j)** The polynomial kernel is actually defined by $\\kappa(\\mathbf{x},\\mathbf{z}) = (\\mathbf{x}\\cdot\\mathbf{z}+r)^d$, but by default this r is 0. Take $r = 1$ and $d = 3$ and list the features in the constructed feature space $\\phi(\\mathbf{x})$ corresponding to this kernel. Is the necessary feature present now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ##### YOUR CODE STARTS ##### (please do not delete this line)\n",
    "    raise NotImplementedError(\"You need to remove \\\"raise NotImplementedErrors\\\" lines after writing your code!\")\n",
    "    ##### YOUR CODE ENDS ##### (please do not delete this line)\n",
    "except NotImplementedError as e:\n",
    "    print(CRED,\"TODO:\",e,CEND)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (j):</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(k)** The value $r$ is represented by parameter 'coef0' in the SVC function. Change it to 1 and see if you can now learn XOR with polynomial kernel of degree 3. Make the resulting plot and report if learning of XOR succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ##### YOUR CODE STARTS ##### (please do not delete this line)\n",
    "    raise NotImplementedError(\"You need to remove \\\"raise NotImplementedErrors\\\" lines after writing your code!\")\n",
    "    ##### YOUR CODE ENDS ##### (please do not delete this line)\n",
    "except NotImplementedError as e:\n",
    "    print(CRED,\"TODO:\",e,CEND)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (k):</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Gaussian kernel in SVM (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(a)** Make __6__ plots and show how RBF kernel works with different C values: 0.1, 0.5, 1, 2, 10, 100. What do you observe? What does parameter C do? \n",
    "\n",
    "    - Hint 1: Remember the soft-margin SVM formula and check out what the C parameter stood for. \n",
    "    - Hint 2: Use `plt.subplots(2, 3, figsize=...)` to arrange them nicely and `plot_svm(svm_model, train_X, train_Y, ax=ax[i][j])` to plot to the subplot in row i and column j. \n",
    "    - Hint 3: Make sure you run `plt.show()` after the plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ##### YOUR CODE STARTS ##### (please do not delete this line)\n",
    "    raise NotImplementedError(\"You need to remove \\\"raise NotImplementedErrors\\\" lines after writing your code!\")\n",
    "    ##### YOUR CODE ENDS ##### (please do not delete this line)\n",
    "except NotImplementedError as e:\n",
    "    print(CRED,\"TODO:\",e,CEND)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (a):</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** Now change the gamma parameter: 0.01, 0.5, 1, 2, 10, 100 and do the same as in the subtask **(a)**. What does the gamma parameter do? (Hint: check out the Gaussian kernel formula and rememeber that $\\gamma = \\frac{1}{2\\sigma}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ##### YOUR CODE STARTS ##### (please do not delete this line)\n",
    "    raise NotImplementedError(\"You need to remove \\\"raise NotImplementedErrors\\\" lines after writing your code!\")\n",
    "    ##### YOUR CODE ENDS ##### (please do not delete this line)\n",
    "except NotImplementedError as e:\n",
    "    print(CRED,\"TODO:\",e,CEND)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (b):</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Decision tree learning (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this task is to understand better how the parameter values affect the performance of decision trees. We will do this on the dataset 'adult.data.csv' originating from https://archive.ics.uci.edu/ml/datasets/adult . This data file has been packaged together with the notebook. The following code loads the dataset and prepares it for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install graphiz package for decision tree visualization in case it is not installed**\n",
    "!pip install graphviz\n",
    "\n",
    "#### In case you still have some issues with graphviz package:\n",
    "**For Windows Users**: You can download graphviz from [<a href = \"https://graphviz.gitlab.io/_pages/Download/Download_windows.html\">HERE</a>] Then add it to the system path using the following commands. <br>\n",
    "```\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'```\n",
    "\n",
    "**For Ubuntu Linux Users:** try to use the following command to install graphviz\n",
    "```sudo apt-get install graphviz```\n",
    "\n",
    "**For MAC Users:** Use the following package\n",
    "```brew install graphviz```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "data = pd.read_csv(\"adult.data.csv\", dtype=\"category\", header = None, index_col = None, na_values=[\"NA\", \" ?\", \"\"])\n",
    "data.columns = [\"age\", \"workclass\", \"not_needed1\", \"education1\", \"education\", \"marital_status\",\n",
    "               \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\",\n",
    "               \"hours_per_week\", \"country\", \"income\"]\n",
    "data.dropna(inplace = True)\n",
    "data[\"age\"] = pd.to_numeric(data[\"age\"])\n",
    "data[\"education\"] = pd.to_numeric(data[\"education\"])\n",
    "data[\"capital_gain\"] = pd.to_numeric(data[\"capital_gain\"])\n",
    "data[\"capital_loss\"] = pd.to_numeric(data[\"capital_loss\"])\n",
    "data[\"hours_per_week\"] = pd.to_numeric(data[\"hours_per_week\"])\n",
    "data[\"capital_gain\"] = data[\"capital_gain\"] - data[\"capital_loss\"]\n",
    "data[\"income\"] = data.income.str.strip()\n",
    "data.drop([\"not_needed1\", \"education1\", \"relationship\", \"capital_gain\", \"capital_loss\",\n",
    "          \"country\"], axis = 1, inplace = True)\n",
    "a = data[data.income == \">50K\"].index\n",
    "b = data[data.income == \"<=50K\"].index\n",
    "b = np.random.choice(b, size = len(a), replace = False)\n",
    "data = data.loc[np.concatenate([a, b])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree implementation in sklearn requires all features to be numeric. We will therefore create a new binary feature for each value of each textual variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns = [\"workclass\", \"marital_status\", \"occupation\", \"race\", \"sex\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider the binary classification task of predicting income:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.income.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.drop([\"income\"], axis = 1),\n",
    "                                                    data[\"income\"], test_size = 0.5, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(a)** First build a decision tree of maximum depth 2, calculate train and test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ##### YOUR CODE STARTS ##### (please do not delete this line)\n",
    "    raise NotImplementedError(\"You need to remove \\\"raise NotImplementedErrors\\\" lines after writing your code!\")\n",
    "    # fit the model here:\n",
    "    dt = DecisionTreeClassifier()\n",
    "    dt.fit()\n",
    "    ##### YOUR CODE ENDS ##### (please do not delete this line)\n",
    "except NotImplementedError as e:\n",
    "    print(CRED,\"TODO:\",e,CEND)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (a):</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** Plot the tree using the following code. Interpret the tree by verbally explaining what decisions it makes on different feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    dot_data = tree.export_graphviz(dt, out_file=None,\n",
    "                                    feature_names=X_train.columns,  \n",
    "                                    class_names=[\"<=50K\", \">50K\"],  \n",
    "                                    filled=True, rounded=True, \n",
    "                                    special_characters=False) \n",
    "    graph = graphviz.Source(dot_data)\n",
    "    graph\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (b):</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Explain how decision tree deals with:\n",
    "\n",
    "1. binary features\n",
    "2. categorical features\n",
    "3. continuous features\n",
    "\n",
    "by describing what all possible splits are that the learning algorithm considers in case of these three types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (c):</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(d)** Next, build a tree with the default parameters and calculate the train and test accuracy. Is the model good? Why/why not? Explain what are the default parameters used in sklearn for decision tree and how they influence this kind of result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ##### YOUR CODE STARTS ##### (please do not delete this line)\n",
    "    raise NotImplementedError(\"You need to remove \\\"raise NotImplementedErrors\\\" lines after writing your code!\")\n",
    "    ##### YOUR CODE ENDS ##### (please do not delete this line)\n",
    "except NotImplementedError as e:\n",
    "    print(CRED,\"TODO:\",e,CEND)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (d):</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(e)** Modify the learning algorithm to get rid of the problem you had in the previous step. Try to change at least 3 parameters (first one at a time, then all together, try different combinations). Report the three parameters you experimented with and explain how they can be used to improve the tree (what do they do?). Which values did you try and what gave the best results? Build the final tree (don't show the experimenting code) and report train and test accuracies. You should achieve test accuracy > 79% without too much overfitting (meaning that training accuracy should be quite similar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (e):</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(f)** Now build a tree with max_depth = 7 and visualize it. Explain how to classify the first testing instance X_test.iloc[0] by visually exploring the tree (list all the nodes visited along the path through the tree until the decision). Is the decision the same as the true label? Does the decision path make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ##### YOUR CODE STARTS ##### (please do not delete this line)\n",
    "    raise NotImplementedError(\"You need to remove \\\"raise NotImplementedErrors\\\" lines after writing your code!\")\n",
    "    ##### YOUR CODE ENDS ##### (please do not delete this line)\n",
    "except NotImplementedError as e:\n",
    "    print(CRED,\"TODO:\",e,CEND)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (f):</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(g)** What seem to be the most important features? Check out the feature importances given by the sklearn model from **(f)**. Do they agree with your guess? How are these feature importances calculated? Hint: feature importances are given by `dt.feature_importances_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ##### YOUR CODE STARTS ##### (please do not delete this line)\n",
    "    raise NotImplementedError(\"You need to remove \\\"raise NotImplementedErrors\\\" lines after writing your code!\")\n",
    "    ##### YOUR CODE ENDS ##### (please do not delete this line)\n",
    "except NotImplementedError as e:\n",
    "    print(CRED,\"TODO:\",e,CEND)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='purple'>Answer to (g):</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <font color='red'>This was the last task! Please restart and run all before submission!</font>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
